{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete code implementation of Vector representation\n",
    "Topics will be covered in this notebook: <br>\n",
    "<ul>\n",
    "<li>Tokenization</li>\n",
    "<li>Bag of word</li>\n",
    "<li>TF/IDF</li>\n",
    "<li>Embedding layer</li>\n",
    "<li>Stemming/lemmatization</li>\n",
    "<li>Stopwords handling</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# VECTOR REPRESENTATION USING SKLEARN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines in train data :  16000\n",
      "No. of lines in test data :  2000\n",
      "No. of lines in validation data :  2000\n"
     ]
    }
   ],
   "source": [
    "## READING DATA FROM DIRECTORY\n",
    "\n",
    "print(\"No. of lines in train data : \",len(open('Data/train.txt','r').readlines()))\n",
    "print(\"No. of lines in test data : \",len(open('Data/test.txt','r').readlines()))\n",
    "print(\"No. of lines in validation data : \",len(open('Data/val.txt','r').readlines()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READING DATA FROM DIRECTORY\n",
    "\n",
    "train = open('data/train.txt','r').readlines()\n",
    "test = open('data/test.txt','r').readlines()\n",
    "val = open('data/val.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CONCATENATING ALL THE DATASETS\n",
    "\n",
    "full_data = train + test + val\n",
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINING X AND Y VARIABLES, (INDEPENDENT AND DEPENDENT)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for item in full_data:\n",
    "    text,label = item.split(';')\n",
    "    label = label.replace('\\n','')\n",
    "    x.append(text)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X and Y var : 20000 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"length of X and Y var :\",len(x),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i didnt feel humiliated',\n",
       " 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'im grabbing a minute to post i feel greedy wrong',\n",
       " 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       " 'i am feeling grouchy',\n",
       " 'ive been feeling a little burdened lately wasnt sure why that was',\n",
       " 'ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny',\n",
       " 'i feel as confused about life as a teenager or as jaded as a year old man',\n",
       " 'i have been with petronas for years i feel that petronas has performed well and made a huge profit',\n",
       " 'i feel romantic too']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RAW TEXT \n",
    "x[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT-CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['didnt feel humili',\n",
       " 'go feel hopeless damn hope around someon care awak',\n",
       " 'im grab minut post feel greedi wrong',\n",
       " 'ever feel nostalg fireplac know still properti',\n",
       " 'feel grouchi',\n",
       " 'ive feel littl burden late wasnt sure',\n",
       " 'ive take milligram time recommend amount ive fallen asleep lot faster also feel like funni',\n",
       " 'feel confus life teenag jade year old man',\n",
       " 'petrona year feel petrona perform well made huge profit',\n",
       " 'feel romant']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we will do lowerise all the text\n",
    "# second we will perform word tokenization\n",
    "# third we will remove stopwords\n",
    "# fourth we will perform stemming or lemmataization on each word\n",
    "# then make a clean text \n",
    "stem = PorterStemmer()\n",
    "\n",
    "def text_cleaning(sentences):\n",
    "    clean_text = []\n",
    "    for sent in sentences:\n",
    "        lower_sent = sent.lower()   # first lowerising the sentence\n",
    "        word_tokenize = nltk.word_tokenize(sent)   #performing word_tokenization\n",
    "        removed_stop_words = [word for word in word_tokenize if word not in stopwords.words('english')]  # remove stopwords\n",
    "        stemmed_sent = [stem.stem(word) for word in removed_stop_words]  # apply stemming\n",
    "        cleaned = \" \".join(stemmed_sent)  # joining our final words\n",
    "        clean_text.append(cleaned)      # appending cleaned text in a separate list\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "CLEAN_TEXT = text_cleaning(x)\n",
    "CLEAN_TEXT[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   TOKENIZATION   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## creating the object Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<nothing>')\n",
    "# we can also pass num_word parameter\n",
    "\n",
    "#we are fitting tokenizer on our cleaned dataset\n",
    "tokenizer.fit_on_texts(CLEAN_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OCCURENCES OF WORD IN CORPUS\n",
    "# tokenizer.word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TOTAL NO. OF DOCUMENTS,\n",
    "tokenizer.document_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[61, 2, 522],\n",
       " [10, 2, 419, 682, 67, 50, 60, 96, 1229],\n",
       " [4, 1230, 431, 107, 2, 432, 192],\n",
       " [92, 2, 592, 3696, 7, 21, 2844],\n",
       " [2, 918]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TOKENIZATION, CONVERT THE TEXT INTO VECTOR \n",
    "sequences = tokenizer.texts_to_sequences(CLEAN_TEXT)\n",
    "sequences[0:5]\n",
    "\n",
    "# AS YOU CAN SEE, ALL ARE WORDS GOT IT UNIQUE NUMBER,\n",
    "# BUT THIS INPUT HAS NO EQUAL LENGTH,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_sentence_length = max(list(map(len,CLEAN_TEXT)))\n",
    "maximum_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  61,    2,  522,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  10,    2,  419,  682,   67,   50,   60,   96, 1229,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [   4, 1230,  431,  107,    2,  432,  192,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  92,    2,  592, 3696,    7,   21, 2844,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [   2,  918,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO EQUALIZE THE VECTOR LENGTH, USING PAD SEQUENCES\n",
    "## SO WE ARE ADDING 0,0.. IN ENDING OF THE VECTOR TO EQUALIZE THE LENGTH\n",
    "## we are using maxlen=35, for an example because actually, its to high -> 208\n",
    "\n",
    "sequences = pad_sequences(sequences,padding='post',maxlen=35)\n",
    "sequences[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTOR REPRESENTATION USING SKLEARN\n",
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['didnt feel humili',\n",
       " 'go feel hopeless damn hope around someon care awak',\n",
       " 'im grab minut post feel greedi wrong',\n",
       " 'ever feel nostalg fireplac know still properti',\n",
       " 'feel grouchi']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEAN_TEXT[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x11578 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 179160 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# HYPERPARAMETERS FOR CountVectorizer\n",
    "# stop_words='english'\n",
    "# max_features=1000\n",
    "# ngram_range=(1, 2)\n",
    "\n",
    "# Fit and transform the corpus to create a document-term matrix\n",
    "BOW = vectorizer.fit_transform(CLEAN_TEXT)\n",
    "BOW \n",
    "## we got a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11578)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature names (words)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Convert the document-term matrix to a dense array for better visualization\n",
    "dense_array = BOW.toarray()\n",
    "## To GET DENSE ARRAY\n",
    "dense_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AS YOU CAN SEE, CONVERTED RAW TEXT INTO NUMERICAL REPRESENTATION\n",
    "dense_array[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Matrix:\n",
    "In a dense matrix, most of the elements are nonzero.\n",
    "All elements, including zero values, are stored in memory.\n",
    "\n",
    "Sparse Matrix:\n",
    "In a sparse matrix, the majority of elements are zero.\n",
    "Only nonzero elements are stored along with their indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF\\IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TFIDF VECTOR REPRESENTATION USING SKLEARN\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vector = tfidf.fit_transform(CLEAN_TEXT).toarray()\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBADDING LAYER - (word embedding)\n",
    "* word2vec\n",
    "* glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['didnt feel humili',\n",
       " 'go feel hopeless damn hope around someon care awak',\n",
       " 'im grab minut post feel greedi wrong',\n",
       " 'ever feel nostalg fireplac know still properti',\n",
       " 'feel grouchi']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THIS IS OUR PREVIOUS CLEAN TEXT \n",
    "CLEAN_TEXT[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1689, 150, 9845],\n",
       " [6801, 150, 5832, 4965, 1011, 933, 5519, 633, 5800],\n",
       " [6074, 47, 1381, 8051, 150, 5654, 9577],\n",
       " [537, 150, 7461, 7831, 727, 9389, 6445],\n",
       " [150, 4621]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## one hot representation with vocab_size=10000, i defined manually\n",
    "## but vocab_size = len(tokenizer.word_index) <- means unique words\n",
    "\n",
    "vocab_size = 10000\n",
    "one_hot_repre = [one_hot(sent,vocab_size) for sent in CLEAN_TEXT]\n",
    "one_hot_repre[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1689,\n",
       "         150, 9845],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 6801,  150, 5832, 4965, 1011,  933, 5519,\n",
       "         633, 5800]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PAD_SEQUENCE\n",
    "\n",
    "sequences = pad_sequences(one_hot_repre,padding='pre',maxlen=35)\n",
    "# maxlen  --> maximum sequence input length\n",
    "sequences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 35)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ranjit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 35, 20)            200000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200000 (781.25 KB)\n",
      "Trainable params: 200000 (781.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum number of unique words in your vocabulary\n",
    "\n",
    "# Create a Sequential model with an Embedding layer\n",
    "model = Sequential()\n",
    "# Add the Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=20, input_length=35))\n",
    "# Display the model summary\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 35, 20)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.predict(sequences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of our previous data :  (20000, 35)\n",
      "shape of Now :  (20000, 35, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of our previous data : \",sequences.shape)\n",
    "print(\"shape of Now : \",embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 20)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.04378361,  0.03708205,  0.02849576,  0.00590458,  0.04794595,\n",
       "        -0.00033931, -0.04129771, -0.02708526,  0.04577501,  0.0384303 ,\n",
       "        -0.0343819 , -0.02140981,  0.0269854 ,  0.0224081 ,  0.01456524,\n",
       "        -0.00935118,  0.02094496, -0.01622526,  0.0063672 , -0.03069818],\n",
       "       [-0.00629985, -0.00684075, -0.02248571,  0.00902597,  0.0367577 ,\n",
       "         0.00891449,  0.00957698,  0.02512175,  0.02995079,  0.02518077,\n",
       "         0.00088751, -0.03109499,  0.00120294,  0.01425451, -0.04484493,\n",
       "         0.02496668,  0.04891412, -0.03390459,  0.02513858, -0.01408529],\n",
       "       [-0.01909895, -0.04500598, -0.0187013 , -0.03490086,  0.02644725,\n",
       "        -0.00399865, -0.02369053,  0.02731479,  0.01576027,  0.00193807,\n",
       "        -0.03744299, -0.00757637, -0.03123112, -0.04408816, -0.02099812,\n",
       "        -0.04471582, -0.01358836,  0.03232244, -0.03790381, -0.00411195],\n",
       "       [ 0.02451302,  0.02531676,  0.03711683, -0.03870156,  0.04285527,\n",
       "         0.02607537, -0.02294756,  0.02509408,  0.0412774 , -0.02658446,\n",
       "         0.00398389, -0.04800439, -0.00304586,  0.04457707, -0.01591843,\n",
       "        -0.02226932,  0.04523729,  0.04799532,  0.02628744, -0.03203151],\n",
       "       [ 0.02703976, -0.00088014, -0.00246047, -0.03875468, -0.04994649,\n",
       "         0.02169016, -0.00502377, -0.02590432, -0.03605292, -0.01910315,\n",
       "        -0.00060209, -0.04385283,  0.04974825, -0.033734  , -0.04745976,\n",
       "         0.0349942 , -0.04551395,  0.01890364,  0.04406532, -0.02683013],\n",
       "       [-0.02276068, -0.03153894,  0.02414152,  0.04488447,  0.0331176 ,\n",
       "        -0.03787638, -0.0217397 ,  0.00088215,  0.01622068, -0.03164374,\n",
       "         0.0296356 ,  0.02036953,  0.04436382, -0.0257349 ,  0.01418575,\n",
       "        -0.00109767, -0.01148394, -0.01917353,  0.00529291,  0.03666029],\n",
       "       [-0.00402652, -0.04859502, -0.02561526, -0.03603669, -0.02277867,\n",
       "        -0.03612589,  0.03521738,  0.00800612, -0.03445265, -0.04717658,\n",
       "         0.04145965, -0.01608038, -0.03535829, -0.00163375,  0.01127113,\n",
       "         0.04422818,  0.03241438, -0.03809949,  0.04595313, -0.02876228],\n",
       "       [-0.03477018,  0.03790336, -0.02326821,  0.02414613,  0.00665641,\n",
       "         0.03602016, -0.03158619, -0.03759656,  0.01234413,  0.02295006,\n",
       "         0.00446408, -0.04258016, -0.00712597, -0.04519763,  0.00496312,\n",
       "        -0.04804585, -0.04856339, -0.03508634,  0.01492354, -0.04399458],\n",
       "       [ 0.02521091,  0.01991595, -0.01631918, -0.04080168,  0.00729381,\n",
       "        -0.0035164 , -0.015873  , -0.04888481, -0.0472943 ,  0.00442492,\n",
       "         0.04725732,  0.0360331 , -0.04084616, -0.025694  , -0.0068692 ,\n",
       "         0.01430904, -0.00878341,  0.03647876, -0.01654933,  0.04579535],\n",
       "       [-0.02699869,  0.01482278,  0.03164959, -0.03263452,  0.033247  ,\n",
       "         0.01358167,  0.01892478, -0.01180314, -0.00965941,  0.02878148,\n",
       "         0.04106071, -0.00348024,  0.02845812, -0.04682132,  0.03358157,\n",
       "        -0.03155929, -0.00203849, -0.00137835,  0.03830732, -0.01222445]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[1]\n",
    "## AS YOU CAN SEE OUR 1ST SENTENCE GOT REPRESENTED IN NUMBER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
